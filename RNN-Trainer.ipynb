{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.externals import *\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import *\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import *\n",
    "from textblob import TextBlob, Word\n",
    "import subprocess\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "from helpers import getTokens, preprocessData, create_csv_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global Variables\n",
    "\n",
    "## Hyper-Parameters\n",
    "FEATURE_SPACE = 10 ## number of embedding features\n",
    "LSTM_LAYERS = 4\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 128\n",
    "ACTIVATION_FUNCTION = \"sigmoid\"\n",
    "CLEAN = False ## determines if we clean the dataset using the cleaning function we have defined\n",
    "\n",
    "## File Paths\n",
    "DATA_POS = 'data/train_pos_full.txt'\n",
    "DATA_NEG = 'data/train_neg_full.txt'\n",
    "VOCAB = 'tools/vocab.pkl'\n",
    "TEST = 'data/test_data.txt'\n",
    "    \n",
    "PREDICTION_PATH = \"RNN_pred_F%d_L%d_E%d_B%d_%s_c%s.csv\" % (FEATURE_SPACE, LSTM_LAYERS, EPOCHS, BATCH_SIZE, ACTIVATION_FUNCTION, CLEAN)\n",
    "MODEL_PATH = \"RNN_model_F%d_L%d_E%d_B%d_%s_c%s.h5\" % (FEATURE_SPACE, LSTM_LAYERS, EPOCHS, BATCH_SIZE, ACTIVATION_FUNCTION, CLEAN)\n",
    "HISTORY_PATH = \"RNN_hist_F%d_L%d_E%d_B%d_%s_c%s.pkl\" % (FEATURE_SPACE, LSTM_LAYERS, EPOCHS, BATCH_SIZE, ACTIVATION_FUNCTION, CLEAN)\n",
    "\n",
    "## If True, train the model on this notebook - else load model from pickle to generate predictions.\n",
    "TRAIN_MODEL = True\n",
    "\n",
    "# If True, use validation data\n",
    "USE_VALID = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VOCAB, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "        \n",
    "pos_tok = getTokens(DATA_POS, vocab)\n",
    "neg_tok = getTokens(DATA_NEG, vocab)\n",
    "test_data = getTokens(TEST, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare data\n",
    "train_data, train_labels = preprocessData(pos_tok, neg_tok)\n",
    "\n",
    "if USE_VALID:\n",
    "    ## Split training and testing data into training and validation set\n",
    "    train_data, validation_data, train_labels, validation_labels = train_test_split(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = len(max(train_data, key=len))\n",
    "\n",
    "## Add padding for sequences\n",
    "## Prepended to guarantee same input length\n",
    "train_data = sequence.pad_sequences(train_data, maxlen=max_length)\n",
    "test_data = sequence.pad_sequences(test_data, maxlen=max_length)\n",
    "\n",
    "if USE_VALID:\n",
    "    validation_data = sequence.pad_sequences(validation_data, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 10)          1012980   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 10)          840       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 10)          840       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 10)          840       \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,016,351\n",
      "Trainable params: 1,016,351\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Build Model\n",
    "\n",
    "## Embedding layer: Embedding is included as part of the NN instead of a pre-processing step\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(len(vocab), FEATURE_SPACE))\n",
    "\n",
    "## Define activation functions we may use\n",
    "FUNCS = {\"sigmoid\":tf.nn.sigmoid, \"relu\":tf.nn.relu}\n",
    "\n",
    "## Add LSTM layer: Ensures that word ordering and contexts are captured in the network\n",
    "for i in range(LSTM_LAYERS-1):\n",
    "    model.add(keras.layers.LSTM(FEATURE_SPACE, return_sequences=True))\n",
    "model.add(keras.layers.LSTM(FEATURE_SPACE))\n",
    "model.add(keras.layers.Dense(1, activation=FUNCS[ACTIVATION_FUNCTION]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile modle\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 418816/1875000 [=====>........................] - ETA: 1:40:49 - loss: 0.3963 - acc: 0.8123"
     ]
    }
   ],
   "source": [
    "## Train data if necessary\n",
    "if TRAIN_MODEL:\n",
    "    history = model.fit(train_data, train_labels, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "    model.save(MODEL_PATH)\n",
    "    pickle.dump(history.history, HISTORY_PATH)\n",
    "else:\n",
    "    model = keras.models.load_model(MODEL_PATH)\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "if USE_VALID:\n",
    "    results = model.evaluate(validation_data, validation_labels)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate predictions\n",
    "y_pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission\n",
    "y_pred[np.where(y_pred <= 0.5)] = 0\n",
    "y_pred[np.where(y_pred > 0.5)] = 1\n",
    "create_csv_submission(range(1,10001), y_pred, PREDICTION_PATH, False)\n",
    "print(\"Predict at\", PREDICTION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
