{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import random\n",
    "from sklearn import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.externals import *\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import *\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import *\n",
    "from textblob import TextBlob, Word\n",
    "import subprocess\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET_POS = 'train_pos.txt'\n",
    "SUBSET_NEG = 'train_neg.txt'\n",
    "\n",
    "DATA_POS = 'train_pos_full.txt'\n",
    "DATA_NEG = 'train_neg_full.txt'\n",
    "\n",
    "CLEAN_DATA_POS = 'train_pos_full_clean.txt'\n",
    "CLEAN_DATA_NEG = 'train_neg_full_clean.txt'\n",
    "CLEAN_TEST = 'test_data_clean.txt'\n",
    "\n",
    "TEST_DATA = 'test_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "pos = pd.read_csv(DATA_POS, header=None, delimiter=\"\\n\", names=[\"tweets\"])\n",
    "neg = pd.read_csv(DATA_NEG, header=None, delimiter=\"\\n\", names=[\"tweets\"])\n",
    "test = pd.read_csv(TEST_DATA, header=None, delimiter=\"\\n\", names=[\"tweets\"])\n",
    "voc = pd.read_csv('vocab_cut.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; i dunno justin read my mention or not ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>because your logic is so dumb , i won't even c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" &lt;user&gt; just put casper in a box ! \" looved t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; thanks sir &gt; &gt; don't trip lil ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting my brother tmr is the bestest birthda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  <user> i dunno justin read my mention or not ....\n",
       "1  because your logic is so dumb , i won't even c...\n",
       "2  \" <user> just put casper in a box ! \" looved t...\n",
       "3  <user> <user> thanks sir > > don't trip lil ma...\n",
       "4  visiting my brother tmr is the bestest birthda..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250000, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vinco tresorpack 6 ( difficulty 10 of 10 objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glad i dot have taks tomorrow ! ! #thankful #s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-3 vs celtics in the regular season = were fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; i could actually kill that girl i'm so ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; i find that very hard to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  vinco tresorpack 6 ( difficulty 10 of 10 objec...\n",
       "1  glad i dot have taks tomorrow ! ! #thankful #s...\n",
       "2  1-3 vs celtics in the regular season = were fu...\n",
       "3  <user> i could actually kill that girl i'm so ...\n",
       "4  <user> <user> <user> i find that very hard to ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250000, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64476, 1)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(file, drop_dup = True):\n",
    "    \n",
    "    data = pd.read_csv(file, header=None, delimiter=\"\\n\", names=[\"tweet\"])\n",
    "    \n",
    "    if drop_dup:\n",
    "        # Removing duplicates\n",
    "        data.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Put everything to lower case (should be already done but just to be safe)\n",
    "    data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    \n",
    "    # Removing user tags or other html stuff\n",
    "    data['tweet'] = data['tweet'].str.replace('<.*?>','')\n",
    "    \n",
    "    # Removing possible mentions or urls (don't know if it's necessary but might be) \n",
    "    data['tweet'] = data['tweet'].str.replace('@\\w+','')\n",
    "    data['tweet'] = data['tweet'].str.replace('http.?://[^\\s]+[\\s]?','')\n",
    "    \n",
    "    # Removing punctuation and symbols\n",
    "    data['tweet'] = data['tweet'].str.replace('[^\\w\\s]', '')\n",
    "    data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in string.punctuation))\n",
    "    \n",
    "    # Removing non alphabetical character\n",
    "    data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x.isalpha()))\n",
    "    \n",
    "    # Removing characters non longer than 1\n",
    "    data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x for x in x.split() if len(x) > 1))\n",
    "    \n",
    "    # Removing stopwords\n",
    "    sw = stopwords.words('english')\n",
    "    data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n",
    "    \n",
    "    # Removing digits (CAN ALSO BE CONVERTED TO WORDS)\n",
    "    data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x for x in x.split() if not x.isdigit()))\n",
    "    \n",
    "    # Removing words that appear less than 5\n",
    "    word_freq = pd.Series(' '.join(data['tweet']).split()).value_counts()\n",
    "    less_freq = word_freq[word_freq < 5]\n",
    "    data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in less_freq))\n",
    "    \n",
    "    # Removing multiple spaces\n",
    "    data['tweet'] = data['tweet'].apply(lambda x: x.strip())\n",
    "    data['tweet'] = data['tweet'].str.replace(' +',' ')\n",
    "\n",
    "    # Lemmatization (better than stemmatization imho)\n",
    "    data['tweet'] = data['tweet'].apply(lambda x: \" \".join([Word(w).lemmatize() for w in x.split()]))\n",
    "    \n",
    "    if drop_dup:\n",
    "        # Removing duplicates again\n",
    "        data.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spelling correction (taking too much time)\n",
    "#pos_first_clean['tweet'] = pos_first_clean['tweet'].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neg_first_clean['tweet'] = neg_first_clean['tweet'].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, file_name):\n",
    "    \n",
    "    data.to_csv(file_name, header=False, index=False, sep=\" \")\n",
    "    subprocess.call([\"sed -i 's/\\\"//g' \" + file_name], shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pos = clean_data(DATA_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_neg = clean_data(DATA_NEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test = clean_data(TEST_DATA, drop_dup = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(clean_pos, CLEAN_DATA_POS)\n",
    "save_data(clean_neg, CLEAN_DATA_NEG)\n",
    "save_data(clean_test, CLEAN_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploaration (TO FINISH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's follow this approach: choosing the model according to the ratio number of samples / number of words per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Median of the number of words per tweet\n",
    "num_words = pd.DataFrame(train_data)[0].str.split().str.len()\n",
    "np.median(num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests the use of sequence of vectors instead of n-grams and probably CNNs are suitable for this task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Vocabulary with top 20000 words\n",
    "# tokenizer = text.Tokenizer(num_words=20000)\n",
    "# tokenizer.fit_on_texts(train_data.tolist())\n",
    "\n",
    "# # Vectorize train and test data\n",
    "# x_train = tokenizer.texts_to_sequences(train_data.tolist())\n",
    "# x_val = tokenizer.texts_to_sequences(test_data.tolist())\n",
    "\n",
    "# # Add padding for sequences\n",
    "# max_length = len(max(x_train, key=len))\n",
    "# x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "# x_val = sequence.pad_sequences(x_val, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Splitting data (done manually for preserving the same number of positive and negative tweets)\n",
    "# def split_data(pos, neg):\n",
    "\n",
    "#     pos_train, pos_test, neg_train, neg_test = train_test_split(pos, neg, test_size=0.2)\n",
    "\n",
    "#     train_data = np.concatenate((pos_train, neg_train))\n",
    "#     test_data = np.concatenate((pos_test, neg_test))\n",
    "\n",
    "#     train_labels = np.concatenate((np.ones(pos_train.shape[0]), np.zeros(neg_train.shape[0])))\n",
    "#     test_labels = np.concatenate((np.ones(pos_test.shape[0]), np.zeros(neg_test.shape[0])))\n",
    "\n",
    "#     train_data, train_labels = shuffle(train_data, train_labels, random_state=0)\n",
    "#     test_data, test_labels = shuffle(test_data, test_labels, random_state=0)\n",
    "    \n",
    "#     return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using NLTK classifiers (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "pos_data_clean = pd.read_csv(CLEAN_DATA_POS, header=None, delimiter=\"\\n\", names=[\"tweets\"])\n",
    "neg_data_clean = pd.read_csv(CLEAN_DATA_NEG, header=None, delimiter=\"\\n\", names=[\"tweets\"])\n",
    "test_data_clean = pd.read_csv(CLEAN_TEST, header=None, delimiter=\"\\n\", names=[\"tweets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading embeddings created before\n",
    "embeddings = np.load(\"embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building features from text\n",
    "def getFeatures(fileName):\n",
    "    feat_repr = []\n",
    "    with open('vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        with open(fileName) as file:\n",
    "            for line in file:\n",
    "                tokens = [vocab.get(t, -1) for t in line.strip().split()]\n",
    "                #tokens = [t for t in tokens if t >= 0]\n",
    "                if (len(tokens) == 0):\n",
    "                    tokens = [-1]\n",
    "                embed_sum = np.zeros(embeddings.shape[1])\n",
    "                for t in tokens:\n",
    "                    embed_sum = np.sum([embed_sum, embeddings[t]], axis=0)\n",
    "                feat_repr.append(embed_sum/len(tokens))\n",
    "    return feat_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_feat = getFeatures(CLEAN_DATA_POS)\n",
    "neg_feat = getFeatures(CLEAN_DATA_NEG)\n",
    "test_feat = getFeatures(CLEAN_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data and get labels\n",
    "\n",
    "pos_clean = np.array(pos_feat)\n",
    "neg_clean = np.array(neg_feat)\n",
    "test_clean = np.array(test_feat)\n",
    "\n",
    "random.seed(123)\n",
    "random.shuffle(pos_clean)\n",
    "random.seed(123)\n",
    "random.shuffle(neg_clean)\n",
    "\n",
    "X_tr = np.concatenate((pos_clean, neg_clean))\n",
    "y_pos = np.ones(pos_clean.shape[0])\n",
    "y_neg = np.zeros(neg_clean.shape[0])\n",
    "y_tr = np.concatenate((y_pos, y_neg))\n",
    "\n",
    "X_tr, y_tr = shuffle(X_tr, y_tr)\n",
    "\n",
    "X_te = test_clean\n",
    "\n",
    "# X_tr, X_te, y_tr, y_te = train_test_split(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 20)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tr, X_te, y_tr, y_te = split_data(pos_feat, neg_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_feat_df = pd.DataFrame(np.stack(pos_feat))\n",
    "# neg_feat_df = pd.DataFrame(np.stack(neg_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Label\n",
    "# pos_feat_df['label'] = 1\n",
    "# neg_feat_df['label'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = np.concatenate((pos_feat_df, neg_feat_df))\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"dataset\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = np.load(\"dataset.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Splitting data\n",
    "# X = pd.DataFrame(dataset).iloc[:,:20].values\n",
    "# y = pd.DataFrame(dataset).iloc[:,20].values\n",
    "\n",
    "# X_tr, X_te, y_tr, y_te = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing\n",
    "std_scaler = StandardScaler()\n",
    "X_tr = std_scaler.fit_transform(X_tr)\n",
    "X_te = std_scaler.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikanikos/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Training with Random Forest\n",
    "rforest = RandomForestClassifier()\n",
    "\n",
    "rforest.fit(X_tr, y_tr)\n",
    "\n",
    "y_pred = rforest.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "y_pred_norm = (y_pred * 2)-1\n",
    "create_csv_submission(range(1,10001), y_pred_norm, \"rand_for.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.,  1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8613923323643496"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy_score(y_te, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with Naive Bayes\n",
    "bayes = GaussianNB()\n",
    "\n",
    "bayes.fit(X_tr, y_tr)\n",
    "\n",
    "y_pred = bayes.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5677248910997817"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy_score(y_te, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikanikos/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Training with Logistic Regression\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "logistic.fit(X_tr, y_tr)\n",
    "\n",
    "y_pred = logistic.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5719094869098845"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy_score(y_te, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with Linear SVM\n",
    "svm_class = svm.LinearSVC()\n",
    "\n",
    "svm_class.fit(X_tr, y_tr)\n",
    "\n",
    "y_pred = svm_class.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5719625012149112"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy_score(y_te, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorFlow for training a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Splitting data\n",
    "# X = np.concatenate((np.array(pos_feat), np.array(neg_feat)))\n",
    "# y_pos = np.ones(pos.shape[0])\n",
    "# y_neg = np.zeros(pos.shape[0])\n",
    "# y = np.concatenate((y_pos, y_neg))\n",
    "\n",
    "# X, y = shuffle(X, y, random_state=0)\n",
    "\n",
    "# train_data, test_data, train_labels, test_labels = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vocab.pkl', 'rb') as f:\n",
    "#     vocab = pickle.load(f)\n",
    "# # vocab = {k:(v+3) for k,v in vocab.items()} \n",
    "# # vocab[\"<PAD>\"] = 0\n",
    "# # vocab[\"<START>\"] = 1\n",
    "# # vocab[\"<UNK>\"] = 2\n",
    "# # vocab[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting words in tokens\n",
    "\n",
    "def getTokens(fileName):\n",
    "    data_tok = []\n",
    "    with open('vocab.pkl', 'rb') as f:\n",
    "            vocab = pickle.load(f)\n",
    "            with open(fileName) as file:\n",
    "                for line in file:\n",
    "                    tokens = [vocab.get(t, -1) for t in line.strip().split()]\n",
    "                    tokens = [t for t in tokens if t >= 0]\n",
    "                    data_tok.append(tokens)         \n",
    "    return data_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_clean = getTokens(DATA_POS)\n",
    "neg_clean = getTokens(DATA_NEG)\n",
    "test_clean = getTokens(TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data and get labels\n",
    "\n",
    "pos_clean = np.array(pos_clean)\n",
    "neg_clean = np.array(neg_clean)\n",
    "test_clean = np.array(test_clean)\n",
    "\n",
    "random.seed(123)\n",
    "random.shuffle(pos_clean)\n",
    "random.seed(123)\n",
    "random.shuffle(neg_clean)\n",
    "\n",
    "X_tr = np.concatenate((pos_clean, neg_clean))\n",
    "y_pos = np.ones(pos_clean.shape[0])\n",
    "y_neg = np.zeros(neg_clean.shape[0])\n",
    "y_tr = np.concatenate((y_pos, y_neg))\n",
    "\n",
    "train_data, train_labels = shuffle(X_tr, y_tr)\n",
    "\n",
    "test_data = test_clean\n",
    "\n",
    "# train_data, test_data, train_labels, test_labels = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating vocabulary with tokenizer of tensorflow\n",
    "# tokenizer = text.Tokenizer(num_words=20000, split=\" \")\n",
    "# tokenizer.fit_on_texts(train_data.tolist())\n",
    "\n",
    "# # Vectorize train and test data\n",
    "# x_train = tokenizer.texts_to_sequences(train_data.tolist())\n",
    "# x_test = tokenizer.texts_to_sequences(test_data.tolist())\n",
    "\n",
    "# # Add padding for sequences\n",
    "# max_length = len(max(x_train, key=len))\n",
    "# x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "# x_test = sequence.pad_sequences(x_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = len(max(train_data, key=len))\n",
    "\n",
    "# Add padding for sequences\n",
    "train_data = sequence.pad_sequences(train_data, maxlen=max_length)\n",
    "test_data = sequence.pad_sequences(test_data, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 16)          1015760   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_4 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,016,049\n",
      "Trainable params: 1,016,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building simple model\n",
    "\n",
    "with open('vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(len(vocab), 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = int(train_data.shape[0] * 0.4)\n",
    "# x_val = train_data[:threshold]\n",
    "# partial_x_train = train_data[threshold:]\n",
    "\n",
    "# y_val = train_labels[:threshold]\n",
    "# partial_y_train = train_labels[threshold:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2500000/2500000 [==============================] - 99s 39us/step - loss: 0.4375 - acc: 0.7929\n",
      "Epoch 2/50\n",
      "2500000/2500000 [==============================] - 100s 40us/step - loss: 0.4069 - acc: 0.8075\n",
      "Epoch 3/50\n",
      "2500000/2500000 [==============================] - 101s 40us/step - loss: 0.3986 - acc: 0.8119\n",
      "Epoch 4/50\n",
      "2500000/2500000 [==============================] - 101s 41us/step - loss: 0.3918 - acc: 0.8158\n",
      "Epoch 5/50\n",
      "2500000/2500000 [==============================] - 101s 40us/step - loss: 0.3837 - acc: 0.8205\n",
      "Epoch 6/50\n",
      "2500000/2500000 [==============================] - 101s 40us/step - loss: 0.3776 - acc: 0.8239\n",
      "Epoch 7/50\n",
      "2500000/2500000 [==============================] - 102s 41us/step - loss: 0.3728 - acc: 0.8265\n",
      "Epoch 8/50\n",
      "2500000/2500000 [==============================] - 101s 40us/step - loss: 0.3689 - acc: 0.8286\n",
      "Epoch 9/50\n",
      "2500000/2500000 [==============================] - 90s 36us/step - loss: 0.3654 - acc: 0.8306\n",
      "Epoch 10/50\n",
      "2500000/2500000 [==============================] - 93s 37us/step - loss: 0.3623 - acc: 0.8322\n",
      "Epoch 11/50\n",
      "2500000/2500000 [==============================] - 93s 37us/step - loss: 0.3593 - acc: 0.8337\n",
      "Epoch 12/50\n",
      "2500000/2500000 [==============================] - 93s 37us/step - loss: 0.3564 - acc: 0.8354\n",
      "Epoch 13/50\n",
      "2500000/2500000 [==============================] - 93s 37us/step - loss: 0.3534 - acc: 0.8370\n",
      "Epoch 14/50\n",
      "2500000/2500000 [==============================] - 93s 37us/step - loss: 0.3506 - acc: 0.8386\n",
      "Epoch 15/50\n",
      "2500000/2500000 [==============================] - 93s 37us/step - loss: 0.3479 - acc: 0.8402\n",
      "Epoch 16/50\n",
      "2500000/2500000 [==============================] - 89s 35us/step - loss: 0.3453 - acc: 0.8417\n",
      "Epoch 17/50\n",
      "2500000/2500000 [==============================] - 92s 37us/step - loss: 0.3428 - acc: 0.8433\n",
      "Epoch 18/50\n",
      "2500000/2500000 [==============================] - 93s 37us/step - loss: 0.3403 - acc: 0.8447\n",
      "Epoch 19/50\n",
      "2500000/2500000 [==============================] - 94s 37us/step - loss: 0.3379 - acc: 0.8460\n",
      "Epoch 20/50\n",
      "2500000/2500000 [==============================] - 94s 38us/step - loss: 0.3355 - acc: 0.8474\n",
      "Epoch 21/50\n",
      "2500000/2500000 [==============================] - 94s 38us/step - loss: 0.3332 - acc: 0.8488\n",
      "Epoch 22/50\n",
      "2500000/2500000 [==============================] - 94s 38us/step - loss: 0.3310 - acc: 0.8498\n",
      "Epoch 23/50\n",
      "2500000/2500000 [==============================] - 94s 37us/step - loss: 0.3289 - acc: 0.8509\n",
      "Epoch 24/50\n",
      "2500000/2500000 [==============================] - 94s 37us/step - loss: 0.3269 - acc: 0.8520\n",
      "Epoch 25/50\n",
      "2500000/2500000 [==============================] - 94s 38us/step - loss: 0.3250 - acc: 0.8531\n",
      "Epoch 26/50\n",
      "2500000/2500000 [==============================] - 94s 38us/step - loss: 0.3231 - acc: 0.8540\n",
      "Epoch 27/50\n",
      "2500000/2500000 [==============================] - 94s 38us/step - loss: 0.3213 - acc: 0.8550\n",
      "Epoch 28/50\n",
      "2500000/2500000 [==============================] - 94s 38us/step - loss: 0.3196 - acc: 0.8558\n",
      "Epoch 29/50\n",
      "2500000/2500000 [==============================] - 88s 35us/step - loss: 0.3179 - acc: 0.8567\n",
      "Epoch 30/50\n",
      "2500000/2500000 [==============================] - 83s 33us/step - loss: 0.3164 - acc: 0.8575\n",
      "Epoch 31/50\n",
      "2500000/2500000 [==============================] - 83s 33us/step - loss: 0.3148 - acc: 0.8584\n",
      "Epoch 32/50\n",
      "2500000/2500000 [==============================] - 83s 33us/step - loss: 0.3133 - acc: 0.8593\n",
      "Epoch 33/50\n",
      "2500000/2500000 [==============================] - 93s 37us/step - loss: 0.3119 - acc: 0.8601\n",
      "Epoch 34/50\n",
      "2500000/2500000 [==============================] - 94s 37us/step - loss: 0.3106 - acc: 0.8606\n",
      "Epoch 35/50\n",
      "2500000/2500000 [==============================] - 94s 38us/step - loss: 0.3093 - acc: 0.8613\n",
      "Epoch 36/50\n",
      "2500000/2500000 [==============================] - 94s 38us/step - loss: 0.3080 - acc: 0.8619\n",
      "Epoch 37/50\n",
      "2500000/2500000 [==============================] - 94s 38us/step - loss: 0.3068 - acc: 0.8626\n",
      "Epoch 38/50\n",
      "2500000/2500000 [==============================] - 102s 41us/step - loss: 0.3057 - acc: 0.8633\n",
      "Epoch 39/50\n",
      "2500000/2500000 [==============================] - 114s 46us/step - loss: 0.3046 - acc: 0.8638\n",
      "Epoch 40/50\n",
      "2500000/2500000 [==============================] - 114s 46us/step - loss: 0.3036 - acc: 0.8642\n",
      "Epoch 41/50\n",
      "2500000/2500000 [==============================] - 114s 46us/step - loss: 0.3026 - acc: 0.8648\n",
      "Epoch 42/50\n",
      "2500000/2500000 [==============================] - 114s 46us/step - loss: 0.3018 - acc: 0.8651\n",
      "Epoch 43/50\n",
      "2500000/2500000 [==============================] - 114s 46us/step - loss: 0.3007 - acc: 0.8658\n",
      "Epoch 44/50\n",
      "2500000/2500000 [==============================] - 114s 46us/step - loss: 0.2999 - acc: 0.8662\n",
      "Epoch 45/50\n",
      "2500000/2500000 [==============================] - 95s 38us/step - loss: 0.2991 - acc: 0.8665\n",
      "Epoch 46/50\n",
      "2500000/2500000 [==============================] - 94s 37us/step - loss: 0.2982 - acc: 0.8670\n",
      "Epoch 47/50\n",
      "2500000/2500000 [==============================] - 106s 43us/step - loss: 0.2974 - acc: 0.8673\n",
      "Epoch 48/50\n",
      "2500000/2500000 [==============================] - 103s 41us/step - loss: 0.2966 - acc: 0.8679\n",
      "Epoch 49/50\n",
      "2500000/2500000 [==============================] - 113s 45us/step - loss: 0.2960 - acc: 0.8682\n",
      "Epoch 50/50\n",
      "2500000/2500000 [==============================] - 113s 45us/step - loss: 0.2953 - acc: 0.8685\n"
     ]
    }
   ],
   "source": [
    "# Training with a neural network\n",
    "history = model.fit(train_data, train_labels, epochs=50, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[np.where(y_pred <= 0.5)] = 0\n",
    "y_pred[np.where(y_pred > 0.5)] = 1\n",
    "y_pred_norm = (y_pred * 2)-1\n",
    "create_csv_submission(range(1,10001), y_pred_norm, \"conv_net_not_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567604/567604 [==============================] - 8s 14us/step\n",
      "[0.4269382586811089, 0.81152000338264]\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "results = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
